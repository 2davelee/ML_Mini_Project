{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#공통 import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "\n",
    "#from joblib import parallel_backend\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#모델 import\n",
    "from xgboost import XGBRFClassifier, XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## randomForest"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 기본"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 세트 정확도: 1.0\n",
      "검증 세트 정확도: 0.9999741662145755\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('./4. 통합 데이터 전처리/df_train_v2.csv')\n",
    "\n",
    "# 입력 데이터, 타깃 데이터, 테스트 데이터 설정\n",
    "data = df_train.drop(columns=['answer', 'num'])\n",
    "target = df_train['answer']\n",
    "\n",
    "# 훈련데이터와 검증데이터로 나누기\n",
    "X_train, X_val, y_train, y_val = train_test_split(data, target, test_size=0.2, random_state=123, stratify=target)\n",
    "\n",
    "# 모델 객체 생성\n",
    "xgb = XGBRFClassifier()\n",
    "\n",
    "# 모델 훈련\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "# 훈련 세트 정확도 출력\n",
    "train_pred = xgb.predict(X_train)\n",
    "train_accuracy = accuracy_score(y_train, train_pred)\n",
    "print(\"훈련 세트 정확도:\", train_accuracy)\n",
    "\n",
    "# 검증 세트 정확도 출력\n",
    "val_pred = xgb.predict(X_val)\n",
    "val_accuracy = accuracy_score(y_val, val_pred)\n",
    "print(\"검증 세트 정확도:\", val_accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. max_depth = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 세트 정확도: 0.9996835320407662\n",
      "검증 세트 정확도: 0.9997158283603297\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('./4. 통합 데이터 전처리/df_train_v2.csv')\n",
    "\n",
    "# 입력 데이터, 타깃 데이터, 테스트 데이터 설정\n",
    "data = df_train.drop(columns=['answer', 'num'])\n",
    "target = df_train['answer']\n",
    "\n",
    "# 훈련데이터와 검증데이터로 나누기\n",
    "X_train, X_val, y_train, y_val = train_test_split(data, target, test_size=0.2, random_state=123, stratify=target)\n",
    "\n",
    "# 모델 객체 생성\n",
    "xgb = XGBRFClassifier(max_depth=10)\n",
    "\n",
    "# 모델 훈련\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "# 훈련 세트 정확도 출력\n",
    "train_pred = xgb.predict(X_train)\n",
    "train_accuracy = accuracy_score(y_train, train_pred)\n",
    "print(\"훈련 세트 정확도:\", train_accuracy)\n",
    "\n",
    "# 검증 세트 정확도 출력\n",
    "val_pred = xgb.predict(X_val)\n",
    "val_accuracy = accuracy_score(y_val, val_pred)\n",
    "print(\"검증 세트 정확도:\", val_accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. max_depth=10, 1분단위"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 세트 정확도: 0.9984094587937223\n",
      "검증 세트 정확도: 0.9979169012498592\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('./4. 통합 데이터 전처리/df_train_v3_1m.csv')\n",
    "\n",
    "# 입력 데이터, 타깃 데이터, 테스트 데이터 설정\n",
    "data = df_train.drop(columns=['answer'])\n",
    "target = df_train['answer']\n",
    "\n",
    "# 훈련데이터와 검증데이터로 나누기\n",
    "X_train, X_val, y_train, y_val = train_test_split(data, target, test_size=0.2, random_state=123, stratify=target)\n",
    "\n",
    "# 모델 객체 생성\n",
    "xgb = XGBRFClassifier(n_estimators=100, max_depth=10, n_jobs=1)\n",
    "\n",
    "# 모델 훈련\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "# 훈련 세트 정확도 출력\n",
    "train_pred = xgb.predict(X_train)\n",
    "train_accuracy = accuracy_score(y_train, train_pred)\n",
    "print(\"훈련 세트 정확도:\", train_accuracy)\n",
    "\n",
    "# 검증 세트 정확도 출력\n",
    "val_pred = xgb.predict(X_val)\n",
    "val_accuracy = accuracy_score(y_val, val_pred)\n",
    "print(\"검증 세트 정확도:\", val_accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 세트 정확도: 1.0\n",
      "검증 세트 정확도: 0.99994370003378\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('./4. 통합 데이터 전처리/df_train_v3_1m.csv')\n",
    "\n",
    "# 입력 데이터, 타깃 데이터, 테스트 데이터 설정\n",
    "data = df_train.drop(columns=['answer'])\n",
    "target = df_train['answer']\n",
    "\n",
    "# 스케일링\n",
    "scaler = StandardScaler() #<- 스케일링 방식 변경 가능\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "# 스케일링된 데이터로 모델 훈련 데이터 나누기\n",
    "X_train, X_val, y_train, y_val = train_test_split(data_scaled, target, test_size=0.2, random_state=123, stratify=target)\n",
    "\n",
    "# 모델 객체 생성\n",
    "xgb = XGBRFClassifier(n_estimators=100, max_depth=10, n_jobs=-1, random_state=42) # max_depth 13으로 바꿈\n",
    "\n",
    "# 모델 훈련\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "# 훈련 세트 정확도 출력\n",
    "train_pred = xgb.predict(X_train)\n",
    "train_accuracy = accuracy_score(y_train, train_pred)\n",
    "print(\"훈련 세트 정확도:\", train_accuracy)\n",
    "\n",
    "# 검증 세트 정확도 출력\n",
    "val_pred = xgb.predict(X_val)\n",
    "val_accuracy = accuracy_score(y_val, val_pred)\n",
    "print(\"검증 세트 정확도:\", val_accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. 스케일링 + 시간제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 세트 정확도: 1.0\n",
      "검증 세트 정확도: 0.9985925008444995\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('./4. 통합 데이터 전처리/df_train_v3_1m.csv')\n",
    "\n",
    "# 입력 데이터, 타깃 데이터, 테스트 데이터 설정\n",
    "data = df_train.drop(columns=['answer', 'year', 'month', 'day', 'hour', 'min'])\n",
    "target = df_train['answer']\n",
    "\n",
    "# 스케일링\n",
    "scaler = StandardScaler() #<- 스케일링 방식 변경 가능\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "# 스케일링된 데이터로 모델 훈련 데이터 나누기\n",
    "X_train, X_val, y_train, y_val = train_test_split(data_scaled, target, test_size=0.2, random_state=123, stratify=target)\n",
    "\n",
    "# 모델 객체 생성\n",
    "xgb = XGBRFClassifier(n_estimators=100, max_depth=10, n_jobs=-1, random_state=42) # max_depth 13으로 바꿈\n",
    "\n",
    "# 모델 훈련\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "# 훈련 세트 정확도 출력\n",
    "train_pred = xgb.predict(X_train)\n",
    "train_accuracy = accuracy_score(y_train, train_pred)\n",
    "print(\"훈련 세트 정확도:\", train_accuracy)\n",
    "\n",
    "# 검증 세트 정확도 출력\n",
    "val_pred = xgb.predict(X_val)\n",
    "val_accuracy = accuracy_score(y_val, val_pred)\n",
    "print(\"검증 세트 정확도:\", val_accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 스케일링 + 차원축소"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 세트 정확도: 0.9999859244141037\n",
      "검증 세트 정확도: 0.9867695079382952\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('./4. 통합 데이터 전처리/df_train_v3_1m.csv')\n",
    "\n",
    "# 입력 데이터, 타깃 데이터, 테스트 데이터 설정\n",
    "data = df_train.drop(columns=['answer'])\n",
    "target = df_train['answer']\n",
    "\n",
    "# 스케일링\n",
    "scaler = StandardScaler() #<- 스케일링 방식 변경 가능\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "# PCA를 사용하여 차원 축소\n",
    "pca = PCA(n_components = 10)  # 목표 차원 수로 조정\n",
    "data_pca = pca.fit_transform(data_scaled)\n",
    "\n",
    "# 축소된 데이터로 새로운 데이터프레임 생성\n",
    "df_pca = pd.DataFrame(data_pca, columns=['PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7', 'PC8', 'PC9', 'PC10'])\n",
    "\n",
    "# 축소된 데이터로 모델 훈련 데이터 나누기\n",
    "X_train, X_val, y_train, y_val = train_test_split(df_pca, target, test_size=0.2, random_state=123, stratify=target)\n",
    "\n",
    "# 모델 객체 생성\n",
    "xgb = XGBRFClassifier(n_estimators=100, max_depth=10, n_jobs=-1, random_state=42) # max_depth 13으로 바꿈\n",
    "\n",
    "# 모델 훈련\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "# 훈련 세트 정확도 출력\n",
    "train_pred = xgb.predict(X_train)\n",
    "train_accuracy = accuracy_score(y_train, train_pred)\n",
    "print(\"훈련 세트 정확도:\", train_accuracy)\n",
    "\n",
    "# 검증 세트 정확도 출력\n",
    "val_pred = xgb.predict(X_val)\n",
    "val_accuracy = accuracy_score(y_val, val_pred)\n",
    "print(\"검증 세트 정확도:\", val_accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. 스케일링 + 시간제거 + 차원축소"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 세트 정확도: 0.9326905482440706\n",
      "검증 세트 정확도: 0.9287805427316743\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('./4. 통합 데이터 전처리/df_train_v3_1m.csv')\n",
    "\n",
    "# 입력 데이터, 타깃 데이터, 테스트 데이터 설정\n",
    "data = df_train.drop(columns=['answer', 'year', 'month', 'day', 'hour', 'min'])\n",
    "target = df_train['answer']\n",
    "\n",
    "# 스케일링\n",
    "scaler = StandardScaler() #<- 스케일링 방식 변경 가능\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "# PCA를 사용하여 차원 축소\n",
    "pca = PCA(n_components = 10)  # 목표 차원 수로 조정\n",
    "data_pca = pca.fit_transform(data_scaled)\n",
    "\n",
    "# 축소된 데이터로 새로운 데이터프레임 생성\n",
    "df_pca = pd.DataFrame(data_pca, columns=['PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7', 'PC8', 'PC9', 'PC10'])\n",
    "\n",
    "# 축소된 데이터로 모델 훈련 데이터 나누기\n",
    "X_train, X_val, y_train, y_val = train_test_split(df_pca, target, test_size=0.2, random_state=123, stratify=target)\n",
    "\n",
    "# 모델 객체 생성\n",
    "xgbrfc = XGBRFClassifier(colsample_bynode=0.6650497566550777, gamma= 0.321582764680029, learning_rate= 0.03768717586862382, max_depth= 9, min_child_weight= 9, n_estimators= 263, reg_alpha= 0.08175903194887191, reg_lambda= 0.8735786241067772, scale_pos_weight= 0.9208724005318132, subsample= 0.5305389799274318)\n",
    "# 모델 훈련\n",
    "xgbrfc.fit(X_train, y_train)\n",
    "\n",
    "# 훈련 세트 정확도 출력\n",
    "train_pred = xgbrfc.predict(X_train)\n",
    "train_accuracy = accuracy_score(y_train, train_pred)\n",
    "print(\"훈련 세트 정확도:\", train_accuracy)\n",
    "\n",
    "# 검증 세트 정확도 출력\n",
    "val_pred = xgbrfc.predict(X_val)\n",
    "val_accuracy = accuracy_score(y_val, val_pred)\n",
    "print(\"검증 세트 정확도:\", val_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
